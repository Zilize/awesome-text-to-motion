[
    {
        "arxiv_id": "non-arxiv1",
        "title": "Generating Diverse and Natural 3D Human Motions from Text",
        "abstract": "Automated generation of 3D human motions from text is a challenging problem. The generated motions are expected to be sufficiently diverse to explore the text-grounded motion space, and more importantly, accurately depicting the content in prescribed text descriptions. Here we tackle this problem with a two-stage approach: text2length sampling and text2motion generation. Text2length involves sampling from the learned distribution function of motion lengths conditioned on the input text. This is followed by our text2motion module using temporal variational autoencoder to synthesize a diverse set of human motions of the sampled lengths. Instead of directly engaging with pose sequences, we propose motion snippet code as our internal motion representation, which captures local semantic motion contexts and is empirically shown to facilitate the generation of plausible motions faithful to the input text. Moreover, a large-scale dataset of scripted 3D Human motions, HumanML3D, is constructed, consisting of 14,616 motion clips and 44,970 text descriptions.\n\n",
        "authors": [
            "Chuan Guo",
            "Shihao Zou",
            "Xinxin Zuo",
            "Sen Wang",
            "Wei Ji",
            "Xingyu Li",
            "Li Cheng"
        ],
        "year": 2022,
        "month": 6,
        "url": "https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf",
        "survey": false,
        "survey_abbr": "",
        "model": true,
        "model_abbr": "T2M",
        "dataset": true,
        "dataset_abbr": "HumanML3D",
        "submission": "CVPR",
        "submission_year": "2022",
        "page": "https://ericguo5513.github.io/text-to-motion/",
        "repo": "https://github.com/EricGuo5513/text-to-motion",
        "backbone_tags": "GRU",
        "approach_tags": ""
    },
    {
        "arxiv_id": "non-arxiv2",
        "title": "CLaM: An Open-Source Library for Performance Evaluation of Text-driven Human Motion Generation",
        "abstract": "Text-driven human motion generation, which creates motion sequences based on textual descriptions, has attracted great attention in the communities of multimedia and artificial intelligence. By parsing and comprehending textual information and converting it into specific human movements, it realizes a direct transformation from human semantics to motion sequences. New text-driven human motion generators are springing up to achieve better performance. However, the absence of well-trained evaluators that can effectively estimate the consistency between the text prompts and motions generated by existing generators remains a challenge. To address the above issues, we propose an open-source library with a powerful Contrastive Language-and-Motion (CLaM) pre-training evaluator, which can be employed for evaluating a variety of text-driven human motion generation algorithms. We perform a thorough performance evaluation of the existing algorithms on various metrics, such as R-Precision. As a by-product, we build a large-scale HumanML3D-synthesis dataset, which consists of 14,616 motion sequences and 547,102 textual descriptions, which is ten times larger than the widely-used HumanML3D dataset. The source codes and models for CLaM are available at~https://github.com/SheldongChen/CLaM/.",
        "authors": [
            "Xiaodong Chen",
            "Kunlang He",
            "Wu Liu",
            "Xinchen Liu",
            "Zheng-Jun Zha",
            "Tao Mei"
        ],
        "year": 2024,
        "month": 10,
        "url": "https://dl.acm.org/doi/10.1145/3664647.3685523",
        "survey": false,
        "survey_abbr": "",
        "model": true,
        "model_abbr": "CLaM",
        "dataset": true,
        "dataset_abbr": "HumanML3D-synthesis",
        "submission": "MM",
        "submission_year": "24",
        "page": "",
        "repo": "https://github.com/SheldongChen/CLaM/",
        "backbone_tags": "Transformer",
        "approach_tags": ""
    },
    {
        "arxiv_id": "non-arxiv3",
        "title": "Text Motion Translator: A Bi-Directional Model for Enhanced 3D Human Motion Generation from Open-Vocabulary Descriptions",
        "abstract": "The field of 3D human motion generation from natural language descriptions, known as Text2Motion, has gained significant attention for its potential application in industries such as film, gaming, and AR/VR. To tackle a key challenge in Text2Motion, the deficiency of 3D human motions and their corresponding textual descriptions, we built a novel large-scale 3D human motion dataset, LaViMo, extracted from in-the-wild web videos and action recognition datasets. LaViMo is approximately 3.3 times larger and encompasses a much broader range of actions than the largest available 3D motion dataset. We then introduce a novel multi-task framework TMT (Text Motion Translator), aimed at generating faithful 3D human motions from natural language descriptions, especially focusing on complicated actions and those not existing in the training set. In contrast to prior works, TMT is uniquely regularized by multiple tasks, including Text2Motion, Motion2Text, Text2Text, and Motion2Motion. This multi-task regularization significantly bolsters the model’s robustness and enhances its ability of motion modeling and semantic understanding. Additionally, we devised an augmentation method for the textual descriptions using Large Language Models. This augmentation significantly enhances the model’s capability to interpret open-vocabulary descriptions while generating motions. The results demonstrate substantial improvements over existing state-of-the-art methods, particularly in handling diverse and novel motion descriptions, laying a strong foundation for future research in the field.",
        "authors": [
            "Yijun Qian",
            "Jack Urbanek",
            "Alexander Hauptmann",
            "Jungdam Won"
        ],
        "year": 2024,
        "month": 11,
        "url": "https://eccv.ecva.net/virtual/2024/poster/266",
        "survey": false,
        "survey_abbr": "",
        "model": true,
        "model_abbr": "TMT",
        "dataset": true,
        "dataset_abbr": "LaViMo",
        "submission": "ECCV",
        "submission_year": "2024",
        "page": "",
        "repo": "",
        "backbone_tags": "LLM, VQ-VAE",
        "approach_tags": "Multi-Task"
    },
    {
        "arxiv_id": "non-arxiv4",
        "title": "Towards Emotion-enriched Text-to-Motion Generation via LLM-guided Limb-level Emotion Manipulating\n",
        "abstract": "In the literature, existing studies on text-to-motion generation (TMG) routinely focus on exploring the objective alignment of text and motion, which largely ignore the subjective emotion information, especially the limb-level emotion information. With this in mind, this paper proposes a new Emotion-enriched Text-to-Motion Generation (ETMG) task, aiming to generate motions with the subjective emotion information. Further, this paper believes that injecting emotions into limbs (named intra-limb emotion injection) and ensuring the coordination and coherence of emotional motions after injecting emotion information (named inter-limb emotion disturbance) is rather important and challenging in this ETMG task. To this end, this paper proposes an LL M-guided Limb-level Emotion Manipulating ( L3 EM) approach to ETMG. Specifically, this approach designs an LLM-guided intra-limb emotion modeling block to inject emotion into limbs, followed by a graph-structured inter-limb relation modeling block to ensure the coordination and coherence of emotional motions. Particularly, this paper constructs a coarse-grained Emotional Text-to-Motion (EmotionalT2M) dataset and a fine-grained Limb -level Emotional Text-to-Motion (Limb-ET2M) dataset to justify the effectiveness of the proposed L3EM approach. Detailed evaluation demonstrates the significant advantage of our L3EM approach to ETMG over the state-of-the-art baselines. This justifies the importance of the limb-level emotion information for ETMG and the effectiveness of our L3EM approach in coherently manipulating such information.",
        "authors": [
            "Tan Yu",
            "Jingjing Wang",
            "Jiawen Wang",
            "Jiamin Luo",
            "Guodong Zhou"
        ],
        "year": 2024,
        "month": 10,
        "url": "https://dl.acm.org/doi/10.1145/3664647.3681487",
        "survey": false,
        "survey_abbr": "",
        "model": true,
        "model_abbr": "L3EM",
        "dataset": true,
        "dataset_abbr": "Limb-ET2M",
        "submission": "MM",
        "submission_year": "2024",
        "page": "",
        "repo": "https://github.com/aekx/L3EM",
        "backbone_tags": "CLIP, LLM, Diffusion",
        "approach_tags": "Graph"
    },
    {
        "arxiv_id": "non-arxiv5",
        "title": "M-Adaptor: Text-driven Whole-body Human Motion Generation",
        "abstract": "Text-driven whole-body human motion generation, which involves the creation of motion sequences based on textual descriptions, has attracted much attention in the communities of computer vision and artificial intelligence. It aims to extend text-driven motion generation tasks to accommodate complex whole-body human motions, encompassing facial expressions and hand gestures. Researchers have recently developed large-scale 3D expressive whole-body motion datasets enriched with semantic labels and pose descriptions. Nonetheless, there remains a considerable demand within the community for a straightforward and effective framework for generating and evaluating whole-body human motion based on textual descriptions. To address the above issues, we introduce M-Adaptor, a two-stage Low-Rank Adaptation (LoRA)-based generator for whole-body motion generation tasks, to improve the quality and diversity of body motions, facial expressions, and hand gestures. In particular, it first generates initial coarse-grained body motion tokens from textual prompts to enhance the stability of generated motions, then iterates fine-grained facial expressions with the LoRA-based adaptor to enhance motion expressiveness. Furthermore, we extend the existing state-of-the-art CLaM model to CLaM-H and CLaM-X for evaluation of SMPL-H and SMPL-X based motion generation. Extensive qualitative and quantitative evaluations demonstrate our framework's superior performance, with a significant R-Precision improvement for text-driven whole-body motion generation.",
        "authors": [
            "Alicia Li",
            "Xiaodong Chen",
            "Bohao Liang",
            "Qian Bao",
            "Wu Liu"
        ],
        "year": 2024,
        "month": 11,
        "url": "https://openaccess.thecvf.com/content/CVPR2025W/PVUW/html/Li_M-Adaptor_Text-driven_Whole-body_Human_Motion_Generation_CVPRW_2025_paper.html",
        "survey": false,
        "survey_abbr": "",
        "model": true,
        "model_abbr": "M-Adaptor",
        "dataset": false,
        "dataset_abbr": "",
        "submission": "CVPR Workshop",
        "submission_year": "2025",
        "page": "",
        "repo": "",
        "backbone_tags": "Transformer",
        "approach_tags": "LoRA"
    },
    {
        "arxiv_id": "non-arxiv6",
        "title": "MotionGPT: Human Motion Synthesis with Improved Diversity and Realism via GPT-3 Prompting",
        "abstract": "There are numerous applications for human motion synthesis, including animation, gaming, robotics, or sports science. In recent years, human motion generation from natural language has emerged as a promising alternative to costly and labor-intensive data collection methods relying on motion capture or wearable sensors (e.g., suits). Despite this, generating human motion from textual descriptions remains a challenging and intricate task, primarily due to the scarcity of large-scale supervised datasets capable of capturing the full diversity of human activity.This study proposes a new approach, called MotionGPT, to address the limitations of previous text-based human motion generation methods by utilizing the extensive semantic information available in large language models (LLMs). We first pretrain a doubly text-conditional motion diffusion model on both coarse (\"high-level\") and detailed (\"low-level\") ground truth text data. Then during inference, we improve motion diversity and alignment with the training set, by zero-shot prompting GPT-3 for additional \"low-level\" details. Our method achieves new state-of-the-art quantitative results in terms of Fréchet Inception Distance (FID) and motion diversity metrics, and improves all considered metrics. Furthermore, it has strong qualitative performance, producing natural results. Code is available at https://github.com/humansensinglab/MotionGPT",
        "authors": [
            "Jose Ribeiro-Gomes",
            "Tianhui Cai",
            "Zoltán Á. Milacski",
            "Chen Wu",
            "Aayush Prakash",
            "Shingo Takagi",
            "Amaury Aubel",
            "Daeil Kim",
            "Alexandre Bernardino",
            "Fernando De La Torre"
        ],
        "year": 2024,
        "month": 7,
        "url": "https://ieeexplore.ieee.org/document/10484383",
        "survey": false,
        "survey_abbr": "",
        "model": true,
        "model_abbr": "MotionGPT",
        "dataset": false,
        "dataset_abbr": "",
        "submission": "WACV",
        "submission_year": "2024",
        "page": "",
        "repo": "https://github.com/humansensinglab/MotionGPT",
        "backbone_tags": "LLM, CLIP, Transformer, Diffusion",
        "approach_tags": ""
    },
    {
        "arxiv_id": "non-arxiv7",
        "title": "ReMoGPT: Part-Level Retrieval-Augmented Motion-Language Models",
        "abstract": "Generation of 3D human motion holds significant importance in the creative industry. While recent notable advances have been made in generating common motions, existing methods struggle to generate diverse and rare motions due to the complexity of motions and limited training data. This work introduces ReMoGPT, a unified motion-language generative model that solves a wide range of motion-related tasks by incorporating a multi-modal retrieval mechanism into the generation process to address the limitations of existing models, namely diversity and generalizability. We propose to focus on body-part-level motion features to enable fine-grained text-motion retrieval and locate suitable references from the database to conduct generation. Then, the motion-language generative model is trained with prompt-based question-and-answer tasks designed for different motion-relevant problems. We incorporate the retrieved samples into the prompt, and then perform instruction tuning of the motion-language model, to learn from task feedback and produce promising results with the help of fine-grained multi-modal retrieval. Extensive experiments validate the efficacy of ReMoGPT, showcasing its superiority over existing state-of-the-art methods. The framework performs well on multiple motion tasks, including motion retrieval, generation, and captioning.",
        "authors": [
            "Qing Yu",
            "Mikihiro Tanaka",
            "Kent Fujiwara"
        ],
        "year": 2025,
        "month": 4,
        "url": "https://ojs.aaai.org/index.php/AAAI/article/view/33044",
        "survey": false,
        "survey_abbr": "",
        "model": true,
        "model_abbr": "ReMoGPT",
        "dataset": false,
        "dataset_abbr": "",
        "submission": "AAAI",
        "submission_year": "2025",
        "page": "",
        "repo": "",
        "backbone_tags": "VQ-VAE",
        "approach_tags": "Multi-Task, Retrieval, Fine-grained"
    },
    {
        "arxiv_id": "non-arxiv8",
        "title": "Semantically Consistent Text-to-Motion with Unsupervised Styles",
        "abstract": "Text-to-stylized human motion generation leverages text descriptions for motion generation with fine-grained style control with respect to a reference motion. However, existing approaches typically rely on supervised style learning with labeled datasets, constraining their adaptability and generalization for effective diverse style control. Additionally, they have not fully explored the temporal correlations between motion, textual descriptions, and style, making it challenging to generate semantically consistent motion with precise style alignment. To address these limitations, we introduce a novel method that integrates unsupervised style from arbitrary references into a text-driven diffusion model to generate semantically consistent stylized human motion. The core innovation lies in leveraging text as a mediator to capture the temporal correspondences between motion and style, enabling the seamless integration of temporally dynamic style into motion features. Specifically, we first train a diffusion model on a text-motion dataset to capture the correlation between motion and text semantics. A style adapter then extracts temporally dynamic style features from reference motions and integrates a novel Semantic-Aware Style Injection (SASI) module to infuse these features into the diffusion model. The SASI module computes the semantic correlation between motion and style features based on texts, selectively incorporating style features that align with motion content, ensuring semantic consistency and precise style alignment. Our style adapter does not require a labeled style dataset for training, enhancing adaptability and generalization of style control. Extensive evaluations show that our method outperforms previous approaches in terms of semantic consistency and style expressivity.",
        "authors": [
            "Linjun Wu",
            "Xiangjun Tang",
            "Jingyuan Cong",
            "He Wang",
            "Bo Hu",
            "Xu Gong",
            "Songnan Li",
            "Yuchen Liao",
            "Yiqian Wu",
            "Chen Liu",
            "Xiaogang Jin"
        ],
        "year": 2025,
        "month": 8,
        "url": "https://dl.acm.org/doi/10.1145/3721238.3730641",
        "survey": false,
        "survey_abbr": "",
        "model": true,
        "model_abbr": "SASI",
        "dataset": false,
        "dataset_abbr": "",
        "submission": "SIGGRAPH",
        "submission_year": "2025",
        "page": "https://fivezerojun.github.io/stylization.github.io/",
        "repo": "",
        "backbone_tags": "UNet, CNN, Transformer, Diffusion",
        "approach_tags": ""
    },
    {
        "arxiv_id": "non-arxiv9",
        "title": "SPORT: From Zero-shot Prompts to Real-time Motion Generation",
        "abstract": "Real-time motion generation has garnered significant attention within the fields of computer animation and gaming. Existing methods typically realize motion control via isolated style or content labels, resulting in short, simply motion clips. In this paper, we propose a motion generation framework, called SPORT (“from zero-Shot Prompt tO Real-Time motion generation”), for generating real-time and ever-changing motions using zero-shot prompts. SPORT consists of three primary components: (1) a body-part phase autoencoder that ensures smooth transitions between diverse motions; (2) a body-part content encoder that mitigates semantic gap between texts and motions; (3) a diffusion-based decoder that accelerates the denoising process while enhancing the diversity and realism of motions. Moreover, we develop a prototype for real-time application in Unity, demonstrating that our approach effectively considering the semantic gap caused by abstract style texts and rapidly changing terrains. Through qualitative and quantitative comparisons, we show that SPORT outperforms other approaches in terms of motion quality, style diversity and inference speed.",
        "authors": [
            "Bin Ji",
            "Ye Pan",
            "Zhimeng Liu",
            "Shuai Tan",
            "Xiaokang Yang"
        ],
        "year": 2025,
        "month": 2,
        "url": "https://ieeexplore.ieee.org/document/10891181/",
        "survey": false,
        "survey_abbr": "",
        "model": true,
        "model_abbr": "SPORT",
        "dataset": false,
        "dataset_abbr": "",
        "submission": "TVCG",
        "submission_year": "2025",
        "page": "",
        "repo": "",
        "backbone_tags": "CLIP, Transformer, Diffusion",
        "approach_tags": "MoE, Phase"
    },
    {
        "arxiv_id": "non-arxiv10",
        "title": "UniTMGE: Uniform Text-Motion Generation and Editing Model via Diffusion",
        "abstract": "Current methods have shown promising results in applying diffusion models to motion generation given text input. However, these methods are limited to unimodal inputs and outputs, restricted to motion generation alone, and lacking multimodal control capabilities. To address these issues, we introduce UniTMGE, a text-motion multimodal generation and editing framework based on diffusion. UniTMGE overcomes single-modality limitations, enabling exceptional performance and strong generalization across multiple tasks like text-driven motion generation, motion captioning, motion completion, and multimodal motion editing. UniTMGE comprises three components: UTMV for mapping text and motion into a shared latent space using contrastive learning, a controllable diffusion model customized for the UTMV space, and MCRE for unifying multimodal conditions into CLIP representations, enabling precise multimodal control and flexible motion editing through simple linear operations. We conducted both closed-world experiments and open-world experiments using the Motion-X dataset with detailed text descriptions, with results demonstrating our model's effectiveness and generalizability across multiple tasks.",
        "authors": [
            "Ruoyu Wang",
            "Yangfan He",
            "Tengjiao Sun",
            "Xiang Li",
            "Tianyu Shi"
        ],
        "year": 2025,
        "month": 4,
        "url": "https://ieeexplore.ieee.org/document/10943808",
        "survey": false,
        "survey_abbr": "",
        "model": true,
        "model_abbr": "UniTMGE",
        "dataset": false,
        "dataset_abbr": "",
        "submission": "WACV",
        "submission_year": "2025",
        "page": "",
        "repo": "",
        "backbone_tags": "CLIP, Transformer, Diffusion",
        "approach_tags": "Multi-Task, Editing"
    }
]